---
title: Configuration
description: LLM providers, model tiers, and environment variables
icon: gear
---

All configuration lives in `.env` at your workspace root. The setup wizard (`octo init`) generates this file, or you can create it manually from [`.env.example`](https://github.com/onetest-ai/Octo/blob/main/.env.example).

## Config File Locations

Octo resolves its workspace by walking up from the current directory looking for `.octo/` or `.env`. If none is found, it falls back to platform defaults.

### Resolution Order

1. Walk up from `cwd` — first directory containing `.octo/` or `.env`
2. Platform default (if no project workspace found):

| Platform | Default Location | State Dir |
|---|---|---|
| macOS / Linux | `~` | `~/.octo/` |
| Windows | `%LOCALAPPDATA%/octo` | `%LOCALAPPDATA%/octo/.octo/` |

3. Override with `OCTO_HOME` env var — points to the workspace root

### Config File Precedence

Octo checks two locations for `.env` and `.mcp.json`, loading the first it finds:

| File | Primary (checked first) | Fallback |
|---|---|---|
| `.env` | `.octo/.env` | `<workspace>/.env` |
| `.mcp.json` | `.octo/.mcp.json` | `<workspace>/.mcp.json` |

<Tip>
For global installs (via `uvx` or `uv tool`), keep all config inside `~/.octo/` — create `~/.octo/.env` and `~/.octo/.mcp.json`. This way Octo works from any directory without a project-local workspace.
</Tip>

## LLM Providers

Octo supports 5 providers. You only need to configure **one**.

<Tabs>
  <Tab title="Anthropic">
    The simplest option — direct API access to Claude models.

    ```env
    ANTHROPIC_API_KEY=sk-ant-...
    DEFAULT_MODEL=claude-sonnet-4-5-20250929
    ```
  </Tab>

  <Tab title="AWS Bedrock">
    Uses Claude models via AWS. Requires AWS credentials with Bedrock access.

    ```env
    AWS_REGION=us-east-1
    AWS_ACCESS_KEY_ID=...
    AWS_SECRET_ACCESS_KEY=...
    DEFAULT_MODEL=us.anthropic.claude-sonnet-4-5-20250929-v1:0
    ```

    <Note>
    Bedrock model IDs include a region prefix (`us.`, `eu.`) and version suffix (`:0`).
    </Note>
  </Tab>

  <Tab title="OpenAI">
    ```env
    OPENAI_API_KEY=sk-...
    DEFAULT_MODEL=gpt-4o
    ```
  </Tab>

  <Tab title="Azure OpenAI">
    ```env
    AZURE_OPENAI_API_KEY=...
    AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
    AZURE_OPENAI_API_VERSION=2024-12-01-preview
    DEFAULT_MODEL=gpt-4o
    ```
  </Tab>

  <Tab title="GitHub Models">
    Free tier available with a GitHub PAT.

    ```env
    GITHUB_TOKEN=ghp_...
    DEFAULT_MODEL=github/openai/gpt-4.1
    ```

    GitHub Models auto-routes to the right LangChain class:
    - `github/claude-*` → ChatAnthropic
    - Everything else → ChatOpenAI
  </Tab>
</Tabs>

## Auto-Detection

The model factory auto-detects the provider from the model name:

| Model name pattern | Provider |
|---|---|
| `github/*` | GitHub Models |
| `eu.anthropic.*`, `us.anthropic.*` | AWS Bedrock |
| `claude-*` | Anthropic direct |
| `gpt-*`, `o1-*`, `o3-*`, `o4-*` | OpenAI |
| `gpt-*` + `AZURE_OPENAI_ENDPOINT` set | Azure OpenAI |

Override with `LLM_PROVIDER` if needed:

```env
LLM_PROVIDER=bedrock  # anthropic | bedrock | openai | azure | github
```

## Model Tiers

Octo uses three tiers to balance cost vs quality. Different agents use different tiers:

| Tier | Used For | Example |
|---|---|---|
| **HIGH** | Complex reasoning, architecture, multi-step planning | `claude-opus-4-5-20250929` |
| **DEFAULT** | Supervisor routing, general chat, tool use | `claude-sonnet-4-5-20250929` |
| **LOW** | Summarization, simple workers, cost-sensitive tasks | `claude-haiku-4-5-20251001` |

```env
DEFAULT_MODEL=claude-sonnet-4-5-20250929
HIGH_TIER_MODEL=claude-opus-4-5-20250929
LOW_TIER_MODEL=claude-haiku-4-5-20251001
```

## Model Profiles

Profiles are presets that map tiers to agent roles:

| Profile | Supervisor | Workers | High-tier agents |
|---|---|---|---|
| `quality` | high | default | high |
| `balanced` | default | low | high |
| `budget` | low | low | default |

```env
MODEL_PROFILE=balanced
```

Switch at runtime with `/profile <name>`.

## Agent Directories

Load agents from external projects by pointing to their AGENT.md directories:

```env
AGENT_DIRS=/path/to/project-a/.claude/agents:/path/to/project-b/.claude/agents
```

Colon-separated. Each directory is scanned for `*/AGENT.md` files.

## Middleware Tuning

```env
# Max chars for a single tool result before truncation
TOOL_RESULT_LIMIT=40000

# Context window summarization triggers (whichever fires first)
SUMMARIZATION_TRIGGER_FRACTION=0.7
SUMMARIZATION_TRIGGER_TOKENS=40000

# Tokens of recent history to keep after summarization
SUMMARIZATION_KEEP_TOKENS=8000

# Supervisor per-message char limit
SUPERVISOR_MSG_CHAR_LIMIT=30000

# Timeout for claude -p subprocess calls (seconds)
CLAUDE_CODE_TIMEOUT=2400
```

## MCP Servers

MCP server configuration lives in `.mcp.json` (checked in `.octo/.mcp.json` first, then workspace root). See [`.mcp.json.example`](https://github.com/onetest-ai/Octo/blob/main/.mcp.json.example) for a template, and [MCP Servers](/features/mcp-servers) for management commands.

Octo includes a built-in [MS Teams server](/guides/teams-integration) — add it to `.mcp.json` for chat integration.

## Next Steps

<CardGroup cols={2}>
  <Card title="Telegram Setup" icon="paper-plane" href="/getting-started/telegram-setup">
    Add Telegram transport
  </Card>
  <Card title="Teams Integration" icon="comments" href="/guides/teams-integration">
    Read and send MS Teams messages
  </Card>
  <Card title="Model Profiles" icon="sliders" href="/guides/model-profiles">
    Fine-tune cost vs quality
  </Card>
  <Card title="MCP Servers" icon="plug" href="/features/mcp-servers">
    Connect external tools via MCP
  </Card>
</CardGroup>
